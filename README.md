# ğŸ› ï¸ Upwork Job Scraper

A stealth-powered Python application that scrapes Upwork job listings based on keywords pulled from a Google Sheet and writes job data into another sheet. It's designed to mimic human behavior using headless Chromium with anti-bot techniques, and is triggered via webhook (e.g., from `n8n`).

## ğŸ“‚ Project Structure

```
upwork-job-scrapper/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ runner.py
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ sheets.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ webhook.py
â”œâ”€â”€ credentials/
â”‚   â””â”€â”€ service_account.json
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âœ… Features

- ğŸ” Stealth scraping with `undetected-chromedriver`
- ğŸ” Handles pagination and deduplication
- ğŸ§  Resumes from last scraped job
- ğŸ“Š Google Sheets API (input + output)
- ğŸ”— Triggered via `n8n` webhook
- ğŸ³ Deployable via Coolify (Docker-based)

## ğŸš€ Getting Started (Local)

### 1. Clone the repository

```bash
git clone https://github.com/sabbir073/upwork-job-scrapper.git
cd upwork-job-scrapper
```

### 2. Create a virtual environment

```bash
python -m venv venv
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Setup environment variables

Create a `.env` file:

```
WEBHOOK_URL=https://your-n8n-webhook-url
```

Place your Google Service Account JSON in `credentials/service_account.json`.

### 5. Run the FastAPI server

```bash
uvicorn api:app --reload
```

Then you can trigger the scraper by:

```bash
curl -X POST http://127.0.0.1:8000/start
```

## ğŸ“¦ Deployment with Coolify

Make sure Coolify is configured to build using the included `Dockerfile`.

```dockerfile
FROM python:3.13-slim
WORKDIR /app
COPY . .
RUN pip install --upgrade pip && pip install -r requirements.txt
CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

Make sure `.env` and `credentials/service_account.json` are configured in your container environment.

## ğŸ§ª Testing with Postman

Use `POST http://localhost:8000/start` or your deployed URL.

## ğŸ“„ Required Google Sheets Setup

- ğŸ”‘ One sheet for **input** (`keywords` column)
- ğŸ“„ Another sheet for **output** (job fields: ID, title, URL, etc.)

See `app/sheets.py` for how the fields are pulled and appended.

## ğŸ“Œ Notes

- Use only Chrome v137+ for compatibility with the current ChromeDriver.
- Scraper retries and uses random delays to avoid blocks.
- When deploying to Coolify, ensure Chromium is installed in the Docker image if needed.

## ğŸ¤ Contributing

Pull requests welcome. Please remove any credentials before committing.

## âš ï¸ Disclaimer

Use responsibly and only for authorized scraping under Upworkâ€™s TOS.

## ğŸ“ƒ License

MIT